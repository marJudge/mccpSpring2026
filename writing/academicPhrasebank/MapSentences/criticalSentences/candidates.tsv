1910.10683	However, we opted to create a new data set because prior data sets use a more limited set of filtering heuristics, are not publicly available, and/or are different in scope (e.g.Â are limited to News data (Zellers etÂ al., 2019; Liu etÂ al., 2019c), comprise only Creative Commons content (Habernal etÂ al., 2016), or are focused on parallel training data for machine translation (Smith etÂ al., 2013)).
1910.10683	We mainly consider models that explicitly process an input with an encoder before generating an output with a separate decoder and we focus on transfer learning rather than zero-shot learning.
1910.10683	As a result, we might obtain additional speedup by specifically corrupting spans of tokens rather than corrupting individual tokens in an i.i.d.Â manner.
1910.10683	As a result, there has been relatively little comparison of different pre-training data sets as well as a lack of a â€œstandardâ€ data set used for pre-training.
1910.10683	Similar observations have been made in prior work, e.g.Â Beltagy etÂ al. (2019) found that pre-training BERT on text from research papers improved its performance on scientific tasks.
1910.10683	It also has the practical benefit of being able to monitor â€œdownstreamâ€ performance for the entire duration of training, rather than just during fine-tuning.
1910.10683	We also save checkpoints every 1,00010001{,}0001 , 000 steps rather than every 5,00050005{,}0005 , 000 steps to ensure we have access to the modelâ€™s parameters before it overfits.
1910.10683	Since this is our final set of experiments, we report results on the test set rather than the validation set.
1910.10683	There was only pie to eat, rather than traditional breakfast foods
1910.10683	multirc question:Â Why was Joey surprised the morning he woke up for breakfast? answer:Â There was only pie to eat, rather than traditional breakfast foods paragraph:Â <b>Sent 1:Â </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent 2:Â </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent 3:Â </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent 4:Â </b>One day, Joey and Jimmy went swimming together at their Aunt Julieâ€™s pond.<br><b>Sent 5:Â </b>Joey woke up early in the morning to eat some food before they left.<br><b>Sent 6:Â </b>He couldnâ€™t find anything to eat except for pie!<br><b>Sent 7:Â </b>Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.<br><b>Sent 8:Â </b>After he ate, he and Jimmy went to the pond.<br><b>Sent 9:Â </b>On their way there they saw their friend Jack Rabbit.<br><b>Sent 10:Â </b>They dove into the water and swam for several hours.<br><b>Sent 11:Â </b>The sun was out, but the breeze was cold.<br><b>Sent 12:Â </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent 13:Â </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14:Â </b>When they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent 15:Â </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16:Â </b>The two squirrels ate some food that Joeyâ€™s mom, Jasmine, made and went off to bed.<br>
1910.10683	marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesnâ€™t seem to mind as they later pose on the dance floor with other friends. united havenâ€™t had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom from manchester unitedâ€™s dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game .
1910.10683	summarize:Â marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesnâ€™t seem to mind as they later pose on the dance floor with other friends. united havenâ€™t had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom from manchester unitedâ€™s dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game .
2111.13463	the research is commonly focused on the estimation and utilization of users preferences towards attributesÂ (Gao etÂ al., 2021). Common to these approaches is that the user is explicitly asked about the desired values for a specific product attribute, much in the spirit of slot-filling dialogue systemsÂ (Gao etÂ al., 2018). For example, in the context of looking for a bicycle recommendation, we might have wheel dimensions or the number of gears as attributes in our item collection. In this case, a system might want to ask a question like â€œHow thick should the tires be?â€ or â€œHow many gears should the bike have?â€ However, ordinary users often do not possess this kind of attribute understanding, which might require extensive domain-specific knowledge. Instead, they only know where or how they intend to use the item. For example, a user might only be interested in using this bike for commuting but does not know what attributes might be good for that purpose.
2111.13463	The novel research objective of this work is to generate implicit attribute questions for eliciting user preferences, related to the intended use of items. This stands in contrast to explicit questions that ask about specific item attributes.
2111.13463	In contrast to session-based recommenders, where user preferences are implicit and inferred from interactions, users explicitly express their preferences here using natural language.
2111.13463	There has been an effort to create large datasets consisting of human conversations that can be used as training data. However, non-conversational data is often leveraged, especially when there is a lack of relevant information in the recorded dialoguesÂ (Jannach etÂ al., 2022).
2111.13463	Our proposed approach shares elements of both of retrieval-based and generation-based methods: it generates questions using a sequence-to-sequence model and stores them in a collection that can be queried using retrieval-based methods. However, the task we focus on is fundamentally different. Namely, we are concerned with preference elicitation through the generation of implicit questions based on item usage, rather than simply responding to user queries or generating dialogue. This renders existing approaches inadequate for our task.
2111.13463	In this paper, we have studied the question of how a conversational recommender system can solicit userâ€™s needs through natural language by using indirect questions about how the wanted product will be used. This contrasts with most prior work that considers how to directly ask about desired product attributes.
2111.13463v2	the research is commonly focused on the estimation and utilization of users preferences towards attributesÂ (Gao etÂ al., 2021). Common to these approaches is that the user is explicitly asked about the desired values for a specific product attribute, much in the spirit of slot-filling dialogue systemsÂ (Gao etÂ al., 2018). For example, in the context of looking for a bicycle recommendation, we might have wheel dimensions or the number of gears as attributes in our item collection. In this case, a system might want to ask a question like â€œHow thick should the tires be?â€ or â€œHow many gears should the bike have?â€ However, ordinary users often do not possess this kind of attribute understanding, which might require extensive domain-specific knowledge. Instead, they only know where or how they intend to use the item. For example, a user might only be interested in using this bike for commuting but does not know what attributes might be good for that purpose.
2111.13463v2	The novel research objective of this work is to generate implicit attribute questions for eliciting user preferences, related to the intended use of items. This stands in contrast to explicit questions that ask about specific item attributes.
2111.13463v2	In contrast to session-based recommenders, where user preferences are implicit and inferred from interactions, users explicitly express their preferences here using natural language.
2111.13463v2	There has been an effort to create large datasets consisting of human conversations that can be used as training data. However, non-conversational data is often leveraged, especially when there is a lack of relevant information in the recorded dialoguesÂ (Jannach etÂ al., 2022).
2111.13463v2	Our proposed approach shares elements of both of retrieval-based and generation-based methods: it generates questions using a sequence-to-sequence model and stores them in a collection that can be queried using retrieval-based methods. However, the task we focus on is fundamentally different. Namely, we are concerned with preference elicitation through the generation of implicit questions based on item usage, rather than simply responding to user queries or generating dialogue. This renders existing approaches inadequate for our task.
2111.13463v2	In this paper, we have studied the question of how a conversational recommender system can solicit userâ€™s needs through natural language by using indirect questions about how the wanted product will be used. This contrasts with most prior work that considers how to directly ask about desired product attributes.
2206.04615	aha! moments occur over time, rather than with greater model scale. On the other hand, increasing model training time and increasing model scale often produce similar performance profiles, for instance in the context of double descent (Nakkiran etÂ al., 2021; Nakkiran, 2019; Nakkiran etÂ al., 2020).
2206.04615	Understanding social bias in machine learning systems in general and language models in particular is an important, multifaceted challenge, and it has been the subject of much prior work (Bolukbasi etÂ al., 2016; Keyes, 2018; Spiel etÂ al., 2019; Parrish etÂ al., 2021; Luccioni & Viviano, 2021; Birhane etÂ al., 2021; Talat etÂ al., 2022). A thorough discussion and quantification of bias is beyond the scope of this overview paper.
2206.04615	sometimes misidentifies elements with atomic numbers in the hundreds by their older, placeholder names. For example, element 111 is now known as roentgenium, but before 2004 was known as unununium; likewise, element 112 was known as ununbium until it was renamed copernicium in 2010. For elements 114, 117, and 118, the largest model misidentifies them by their placeholder names rather than their current names (see FigureÂ 17). This is almost certainly a reflection of the fact that training data includes documents written before the renaming of these elements.
2206.04615	Another lesson from this task is that in some cases, the improvement in performance from adding more shots is largely attributable to improving pattern matching; as the performance-vs-scale plot in FigureÂ 17a shows, the largest model achieves similar performance on this task regardless of the number of shots it is presented with. In order to achieve this result, however, we needed to postprocess the modelâ€™s raw output by finding the first element name in the output; without this postprocessing, the model appeared to perform much worse in the zero-shot setting because it would often answer with a sentence like â€˜â€˜The element with atomic number 1 is hydrogenâ€™â€™ rather than simply â€˜â€˜hydrogen.â€™â€™ Care must be taken with purely automatic evaluation of generative tasks.
2206.04615	To address the lack of geographic diversity in NLP, Masakhane used participatory research to construct a machine-translation benchmark for over 30 low-resource languages (Nekoto etÂ al., 2020).
2206.04615	Limitations that we believe will require new approaches, rather than increased scale alone, include an inability to process information across very long contexts (probed in tasks with the keyword context length),
2206.04615	a lack of episodic memory into the training set (not yet directly probed), an inability to engage in recurrent computation
2206.04615	The models we evaluated performed better on English than non-English language tasks. Models performed particularly poorly on tasks involving low resource languages. In several cases, performance on low resource language tasks failed to improve with model scale, even when performance on corresponding English language tasks improved reliably with scale (Figures 14 and 15).
2211.05100	We did not consider mixture-of-experts (MoE) (Shazeer etÂ al., 2017), due to a lack of widely used GPU-based codebases suitable for training them at scale. Similarly, we also did not consider state-space models (Gu etÂ al., 2020). At the time of the design of BLOOM, they consistently underperformed in natural language tasks (Gu etÂ al., 2021). Both of these approaches are promising, and have now demonstrated competitive resultsâ€“at large scales for MoEÂ (Fedus etÂ al., 2022; Srivastava etÂ al., 2022), and at smaller scale for state-space models with H3Â (Fu etÂ al., 2023).
2211.05100	We train six size variants of BLOOM with respective hyperparameters detailed in Table 3. Architecture and training hyperparameters come from our experimental results (Le Scao etÂ al., 2022) and prior work on training large language models (Brown etÂ al., 2020; Kaplan etÂ al., 2020). Model depth and width for the non-176B models roughly follow previous literature (Brown etÂ al., 2020; Zhang etÂ al., 2022), deviating for 3B and 7.1B in order only to fit the models more easily on our training setup. Embedding parameter sizes are larger for BLOOM owing to the larger multilingual vocabulary, but scaling literature discounts embedding operations (Kaplan etÂ al., 2020). During the development process at the 104B parameters scale, we experimented with different values of Adam Î²ğ›½\betaitalic_Î² parameters, weight decay and gradient clipping to target stability, but did not find it helpful. For all models, we use a cosine learning rate decay schedule (Loshchilov and Hutter, 2016) over 410B tokens, taken as an upper bound for the length of training if compute permitted, and warmup for 375M tokens. We use weight decay, gradient clipping, and no dropout. The ROOTS dataset contains around 341 billion tokens of text, so we aimed to train all models for the equivalent amount of tokens. However, in light of revised scaling laws published during training (Hoffmann etÂ al., 2022), we decided to train the large models for an additional 25 billion tokens on repeated data. As warmup tokens + decay tokens were larger than the total number of tokens, the end of learning rate decay was never reached.
2211.05100	Based on recent research on the impact of prompting on language model performance, we decided to build a language model evaluation suite that allowed us to vary both the basic task data as well as the prompting that is used to contextualize the task. Our prompts were developed prior to BLOOMâ€™s release, and did not undergo any a priori refinement using models. That is, the prompts we use in our evaluation are ones that humans believed were a reasonable way to solicit the desired task behavior from a language model. Our goal for designing prompts in this way is to simulate realistic zero-shot or one-shot results that a new user could expect from BLOOM. This is in contrast to presenting best-case performances that might result from multiple rounds of trial-and-error on prompt design. We choose to report the former because the latter is harder to reproduce systematically, is arguably a less representative picture of how the model works in the average setting, and is not representative of true zero-shot learning where no labeled data is available.
2211.05100	These tasks are English-only, and are thus included to facilitate comparison with prior work, which has primarily focused on English-only models. We also note that performance on these tasks has not yet been widely reported using zero- and one-shot prompt-based setting. T0 (Sanh etÂ al., 2022) is the first exception, but that model is instruction-tuned and thus not directly comparable to models like BLOOM and OPT. For each task, we select a random sample of five prompts from promptsource and evaluate all models on that set of prompts.
2211.05100	We evaluate summarization on the WikiLingua (Ladhak etÂ al., 2020) dataset. WikiLingua is a multilingual summarization dataset comprising WikiHow article and step-by-step summary pairs. Pairs are aligned across multiple languages, with translation of source and summary further reviewed by an international translation team. One-shot conditional natural language generation has typically not been reported by models with size comparable to BLOOM. PaLM (Chowdhery etÂ al., 2022) is the first exception, and reports scores on WikiLingua; however, only the modelâ€™s ability to summarize in English was examined (-Â¿ en). By contrast, we opted to test BLOOMâ€™s inherent multilingual ability by assessing the abstractive summarization in the source language (e.g.Â vi -Â¿ vi). We focus on the nine languages (Arabic, English, Spanish, French, Hindi, Indonesian, Portuguese, Vietnamese and Chinese) which were amongst those targeted as part of the BigScience effort.
2211.05100	Natural language generation is notoriously challenging to evaluate, with multilingual generation compounding this challenge due to a lack of metric support. Following the suggestions by Gehrmann etÂ al. (2022b), we report ROUGE-2, ROUGE-L (Lin, 2004),323232For ROUGE, we used the Python implementation at
2211.05100	We perform an additional analysis comparing BLOOM models across model sizes. As a baseline, we also measure the average one-shot accuracy of OPT models of similar sizes (350M parameters to 175B parameters).333333We do not evaluate OPT-66B because of the lack of a similarly-sized BLOOM model. Figure 8 shows the accuracy of each prompt on each task across model scales. Both OPT and BLOOM model families improve very slightly with scale, with only models over 2 billion parameters showing signal, and there is no consistent difference between families across all tasks. In the 1-shot setting, BLOOM-176B is ahead of OPT-175B on Ax-b, CB, WSC and WiC, and matches it on the other tasks, suggesting that multilinguality does not limit performance of BLOOM on English-only tasks in the zero-shot setting.
2211.05100	However, results are very poor between Swahili and Yoruba, languages that are present but under-represented in BLOOMâ€™s training data (<<<50k tokens each). This contrasts with the results for translation between Romance (and therefore related) languages, where results are good across-the-board, including for translation from Galician (glg), a language not included in the training data, but which shares many similarities with the other Romance languages, in particular with Portuguese (por). This however does question BLOOMâ€™s quality on those under-represented low-resource languages included in training.
2211.05100	As discussed in Section 4.1, we report ROUGE-2 scores for the sake of comparability with prior work, and because there is a lack of alternatives for generation evaluation.
2211.05100	Table 14 presents the results per bias type in the CrowS-Pairs dataset. The results are quite homogeneous over the categories, which contrasts with previous studies on masked language models, which suggested models were prone to bias in specific categories, which differed between models tested. Nonetheless, accuracy significantly differs from 50 (T-test, p Â¡ .05) overall for both languages, as well as for a number of bias categories, as shown per asterisks in the table.
2211.05100	Our evaluation of biases in the model are further limited to the situations, languages and language variants that are covered by multilingual CrowS-Pairs.
2303.08774	To test its capabilities in such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In these evaluations it performs quite well and often outscores the vast majority of human test takers. For example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers. This contrasts with GPT-3.5, which scores in the bottom 10%.
2303.08774	All free-response questions consisting of formal essays which required evaluation of writing quality (AP English Language and Composition, AP English Literature and Composition, AP World History, AP US History, AP US Government and Politics, AP Art History, the GRE, and the SAT) were graded by 1-2 qualified third-party contractors with relevant work experience grading those essays. We sampled these responses using a few-shot prompt containing one high-quality sample GRE essay response (which you can also see in section A.8) in order to encourage the model to produce appropriately sophisticated text, rather than an unnaturally terse reply. We graded all other free-response questions on their technical content, according to the guidelines from the publicly-available official rubrics.
2303.08774	Explanation for Problem 1: The painting depicts Nadar, a famous French photographer, raising a camera to the sky, as if to signify the recognition of photography as a legitimate form of art. This recognition would have likely come from a court decision, rather than the other options listed. The first photographic exhibition and the invention of the zoopraxiscope, which was a device used to create moving images, would not have had any bearing on the recognition of photography as art. Similarly, the first use of color photography would not have been a significant enough event to warrant such a celebratory depiction in the painting. Therefore, the most likely explanation for the painting is that it was done in response to a court decision that determined that photographs could be considered works of art.
2303.08774	Explanation for Problem 3: Giovanni Battista Gaulli, also known as Baciccio, was heavily influenced by the Sistine Chapel frescoes in his own works. This is evident in his use of large-scale, dramatic compositions and his focus on religious themes, which were similar to those found in the Sistine Chapel frescoes. In contrast, the other options listed were not as directly influenced by the Sistine Chapel frescoes. Gianlorenzo Bernini was a contemporary of Baciccio, but his works were more focused on classical themes and styles, rather than the religious themes found in the Sistine Chapel frescoes. Peter Paul Rubens was a later artist who was more influenced by the Baroque style, rather than the Renaissance style of the Sistine Chapel frescoes. Rachel Ruysch was a Dutch artist who was not known for large-scale religious works, and therefore would not have been directly influenced by the Sistine Chapel frescoes.
2303.11366	rely on massive models with an enormous number of parameters, such approaches
2303.11366	have been so far limited to using in-context examples as a way of teaching
2303.11366	but is limited to single-generation reasoning tasks. Pryzant etÂ al., (2023)
2303.11366	also limited to single-generation tasks.
2304.08354	Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of recent powerful foundation models, artificial intelligence systems have the potential to be equally adept in tool use as humans. This paradigm, which is dubbed as tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation and comprehensive review of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. We recapitulate existing tool learning research and formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning.
2304.08354	Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 18181818 representative tools and show the potential of current foundation models in skillfully utilizing tools.
2304.08354	Considering the lack of a systematic tool learning evaluation in prior works, we conduct experimentsÂ (Â§Â 4) on 18181818 representative tools based on our framework to investigate the efficacy and limitations of foundation models in tool manipulation. We demonstrate that state-of-the-art foundation models (e.g., ChatGPT) can effectively use tools to solve tasks with simple prompting. These results highlight the potential of using the foundation model as a general agent for tool learning.
2304.08354	Program-based tools are software tools primarily designed for use through programming interfaces rather than visual interfaces.
2304.08354	For instance, generating presentation slides111https://www.microsoft.com/en-us/microsoft-365, constructing 3D models via CAD applications, and scheduling meetings through the analysis of team member calendars are examples of complex tasks that have not been defined in traditional artificial intelligence.
2304.08354	Recent explorations in instruction tuningÂ (Wei etÂ al., 2022a) demonstrate that foundation models can possess extraordinary proficiency in comprehending user instructions. Prior work has shown that fine-tuning large language models on a collection of datasets templated with human instructions allows models to generalize even to instructions for unseen tasksÂ (Wei etÂ al., 2022a; Mishra etÂ al., 2022; Sanh etÂ al., 2022; Bach etÂ al., 2022; Ouyang etÂ al., 2022). Promisingly, such generalization ability can further be enhanced by scaling up both the model size and the quantity or diversity of training instructionsÂ (Iyer etÂ al., 2022). Despite the impressive intent understanding capabilities, challenges still exist in real-world tool learning scenarios:
2304.08354	Humans do not stick to only one single tool to complete complex tasks. Instead, we carefully decompose the task into several sub-tasks, select the most suitable tool for each sub-task, and gradually accomplish them step by step. As discussed above, current research has shown satisfactory performance in task decomposition. However, there is a lack of exploration in utilizing different tools for different sub-tasks. Most of the research mentioned in this section is limited to either multi-step single-tool or single-step multi-tool scenarios.
2304.08354	Guidance, either from humans or environments, plays a critical role in training foundation models to use tools. In contrast to the prompting-based methods mentioned in Â§Â 3.2.1 and Â§Â 3.2.2, which rely on the frozen foundation modelsâ€™ in-context learning abilities, the training-based method optimizes the model with supervision.
2304.08354	To facilitate knowledge transfer among tools, it is critical to design a unified interface that enables the model to manipulate various tools in a consistent and standardized manner, which serves as the foundation for generalizable tool learning. Through a unified interface, models can identify and abstract essential features of tools more easily in a unified tool protocol rather than grappling with the difficulty of understanding various tool interfaces. Currently, the manipulation of tools is through predicting discrete action tokens, and the action space is not aligned in different scenarios, which prohibits the models from quickly adapting to new scenarios and tools. Inspired by the aspect we categorize tools in Â§Â 2.2, we identify three potential ways of interface unification: the semantic interface, GUI interface, and programming interface.
2304.08354	GUI Interface. Humans primarily interact with the digital world through GUI interface (e.g., mouse and keyboard), which has been extensively optimized to follow human action efficiently. Nevertheless, before robots can learn to use a GUI interface flexibly, it is necessary to establish a virtual environment that can facilitate mapping predicted tokens to human-like mouse movements and keyboard inputs. Prior research has explored providing platforms for agents to complete web-based tasks using keyboard and mouse actionsÂ (Shi etÂ al., 2017; Liu etÂ al., 2018a). However, these environments restrict models to a limited set of pre-defined mouse options and common keyboard actions such as copy and paste. By leveraging foundation models, it is possible to introduce prior knowledge regarding common combinations of keyword and mouse actions, thereby expanding the potential actions that a model can execute.
2304.08354	The creation and utilization of tools have traditionally been considered exclusive to human intelligence. However, with the emergence of foundation models, this notion is being challenged. Increasing evidence indicates that the ability to create advanced tools is no longer limited to human beings. For instance, large code modelsÂ (Chen etÂ al., 2021) can generate executable programs based on language description. These programs can be deemed as tools to help accomplish specific tasks. ChatGPT plugins212121https://openai.com/blog/chatgpt-plugins present an awesome example about asking GPT-4 to write a TODO plugin and integrate it with ChatGPT. Besides writing codes from scratch, foundation models can also encapsulate existing tools into stronger tools. In FigureÂ 7, we show an example of ChatGPT encapsulating a weather forecast API into a new function that calculates the average temperature. All such evidence implies the potential for foundation models to transition from merely tool users to tool makers.
2304.08354	While embodied learning emphasizes the use of physical interactions within a simulated environment, tool learning is not limited to a specific environment, but rather focuses on using interfaces that extend the language modelâ€™s capabilities. The intersection between these two paradigms could lead to the development of more advanced AI models capable of learning and adapting in complex and dynamic environments. Here we discuss two possible directions.
2304.08354	Traditional embodied learning learns directly from the environment, where the actions are often atomic and limited to basic tasks such as push, put, and drag, which fall short of the complexity of human problem-solving abilities. To narrow the gap between sim-to-real transferÂ (Kadian etÂ al., 2020) and enhance agent performance, it is essential to incorporate embodied tools within simulated environments. For instance, by introducing objects such as hammers and knives, we can evaluate an agentâ€™s capacity to choose the appropriate tool for cutting a piece of paper. Despite the potential benefits of such tools, to date, no studies have systematically explored the utilization of simulated tools in simulated environments, owing to the complexity of the simulation. Nevertheless, with the rapid growth of computational power in physical engines, such research directions are becoming increasingly practical. A starting point could be utilizing the assets of 3D model that has a more delicate interface and more realistic physical engine support. An additional avenue worth investigating is the automated generation of tools. Given that in tool learning, models can generate functions to define an API for their subsequent utilization, if the embodied agents are capable of generating assets from scratch or composing existing ones within a simulated environment, their intelligence quotient will be further amplified.
2304.08354	Since the aforementioned conflicts can lead to a lack of explainability in model prediction and planning, it is crucial to guide models to integrate tool responses correctly and reliably. Research in open-domain QA has shown that small-scale models like T5Â (Raffel etÂ al., 2020) may rely too heavily on their own knowledge after being fine-tuned on a specific datasetÂ (Longpre etÂ al., 2021). In contrast, more advanced foundation models like ChatGPT handle such issues far better. In FigureÂ 8 and FigureÂ 9, we conduct case studies of ChatGPT (Mar 23, 2023 version) by testing its behavior when conflicts arise. We find that ChatGPT is able to correct its own belief given retrieved information and discern the knowledge conflicts from different sources. Recent studiesÂ (Nakano etÂ al., 2021; Menick etÂ al., 2022) have also attempted to guide models to rely more on augmented knowledge for faithful predictions. However, these works assume that the augmented responses come from a single reliable source, which may not always be the case in more complicated scenarios.
2305.10601	ToT is a framework that empowers LMs to more autonomously and intelligently make decisions and solve problems. While current tasks are limited to reasoning and search problems, future applications involving interaction with external environments or humans could bring potential danger, e.g.â€‰facilitating harmful uses of LMs. On the other hand, ToT also improves the interpretability of model decisions and the opportunity for human alignment, as the resulting representations are readable, high-level language reasoning instead of implicit, low-level token values.
2306.05685	However, there has not been a systematic study of this approach.
2306.05685	While largely overlooked by existing LLM benchmarks, human preferences serve as a direct measure of a chatbotâ€™s utility in open-ended, multi-turn human-AI interactions. To bridge this gap, we introduce two novel benchmarks expressly tailored to assess human preferences. Simultaneously, these benchmarks are designed to distinguish the core capabilities of state-of-the-art models.
2306.05685	Note that in math/coding category, GPT-3.5 and GPT-4 have similar overall win-rate because they both failed to answer some hard questions, but GPT-4 is still significantly better than GPT-3 in the direct pairwise comparison or single-answer grading.
2307.16789	This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT.
2307.16789	Although prior works have explored building instruction tuning data for tool useÂ (Li etÂ al., 2023a; Patil etÂ al., 2023; Tang etÂ al., 2023; Xu etÂ al., 2023b), they fail to fully stimulate the tool-use capabilities within LLMs and have inherent limitations: (1) limited APIs: they either fail to involve real-world APIs (e.g., RESTAPI)Â (Patil etÂ al., 2023; Tang etÂ al., 2023) or consider only a small scope of APIs with poor diversityÂ (Patil etÂ al., 2023; Xu etÂ al., 2023b; Li etÂ al., 2023a);
2307.16789	As illustrated in FigureÂ 1, we collect a high-quality instruction-tuning dataset ToolBench. It is constructed automatically using ChatGPT (gpt-3.5-turbo-16k), which has been upgraded with function call (link) capabilities. The comparison between ToolBench and prior works is listed in TableÂ 1.
2307.16789	Different from prior works, we specifically focus on two crucial aspects for instruction generation: (1) diversity: to train LLMs to handle a wide range of API usage scenarios, thereby boosting their generalizability and robustness; and (2) multi-tool usage: to mirror real-world situations that often demand the interplay of multiple tools, improving the practical applicability and flexibility of LLMs.
2308.11432	Based on the previous studies, we also present several challenges and future directions in this field.
2308.11432	In previous studies, the agents are assumed to act based on simple and heuristic policy functions, and learned in isolated and restricted environmentsÂ [1, 2, 3, 4, 5, 6].
2308.11432	Because of these gaps, the agents obtained from previous studies are usually far from replicating human-level decision processes, especially in unconstrained, open-domain settings.
2308.11432	For the first problem, we present a unified agent framework, which can encompass most of the previous studies.
2308.11432	Drawing from a wealth of previous studies, we identify various challenges in this field and discuss potential future directions.
2308.11432	In contrast to previous studies, where the feedback is given as a scalar value, this model leverages LLMs to provide more detailed verbal feedback, which can provide more comprehensive supports for the agent plans.
2308.11432	Previous studies often utilize external models to expand the range of possible actions.
2308.11432	In contrast to prior studies, the dataset presented in this paper encompasses diverse tasks, real-world scenarios, and comprehensive user interaction patterns.
2308.11432	This section provides a succinct summary of previous studies, categorizing them according to their applications in three distinct areas: social science, natural science, and engineering (see the left part of FigureÂ 5 for a global overview).
2308.11432	In summary, in the above sections, we introduce the typical applications of LLM-based autonomous agents in three important domains. To facilitate a clearer understanding, we have summarized the relationship between previous studies and their respective applications in TableÂ 2.
2308.11432	In contrast to subjective evaluation, objective metrics aim to provide concrete, measurable insights into the agent performance.
2308.11432	In contrast to the aforementioned metrics used to evaluate the agent effectiveness, these metrics aim to assess the efficiency of agent.
2308.11432	In addition, previous researchÂ [30] has shown that existing LLMs may not well model the human cognitive psychology characters, leading to the lack of self-awareness in conversation scenarios.
2308.11432	To ensure rational behavior in agents, itâ€™s a common practice for designers to embed supplementary modules, such as memory and planning modules, into LLMs. However, the inclusion of these modules necessitates the development of more complex prompts in order to facilitate consistent operation and effective communication. Previous researchÂ [183, 184] has highlighted the lack of robustness in prompts for LLMs, as even minor alterations can yield substantially different outcomes. This issue becomes more pronounced when constructing autonomous agents, as they encompass not a single prompt but a prompt framework that considers all modules, wherein the prompt for one module has the potential to influence others.
2308.11432	Hallucination poses a fundamental challenge for LLMs, characterized by the modelsâ€™ tendency to produce false information with a high level of confidence. This challenge is not limited to LLMs alone but is also a significant concern in the domain of autonomous agents. For instance, inÂ [185], it was observed that when confronted with simplistic instructions during code generation tasks, the agent may exhibit hallucinatory behavior. Hallucination can lead to serious consequences such as incorrect or misleading code, security risks, and ethical issues [185]. To mitigate this issue, incorporating human correction feedback directly into the iterative process of human-agent interaction presents a viable approachÂ [23].
2308.16505	Rather than using the step-by-step approach, we adopt a two-phase method. In the first phase, we prompt the LLM to generate a complete tool execution plan based on the userâ€™s intention derived from the dialogue. In the second phase, the LLM strictly adheres to the plan, calling tools in sequence while allowing them to communicate via the Candidate Bus.
2310.10108	Although pioneering studies on agents have detailed the architecture of memory, providing foundational blueprints for subsequent explorations, the emotional memories have been largely overlooked (Park etÂ al., 2023; Wang etÂ al., 2023d).
2310.10108	This stands in contrast to conventional agent memory designs, such as self-summarization (Matelsky etÂ al., 2023), self-verification (Schick etÂ al., 2023), and self-correction (Sel etÂ al., 2023), which predominantly condense or deduce advanced factual knowledge, often sidelining emotional feelings.
2310.10108	Emotions in the recommendation environment can shape an agentâ€™s experience significantly, which are often overlooked in simulations (Wang etÂ al., 2023d; Rohde etÂ al., 2018).
2310.10108	Among these agents, S3superscriptğ‘†3S^{3}italic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT (Gao etÂ al., 2023a) leverages autonomous agents to simulate the dynamic evolution of public opinion on social platforms in response to trending social events. SANDBOX (Liu etÂ al., 2023) facilitates communication among a group of agents to address social issues thus providing ethically sound data for LLM fine-tuning. Differing from task-oriented agents in the field of recommendation, simulation-oriented agents seek to emulate user behaviors within recommender systems rather than focusing on the recommender. RecAgent (Wang etÂ al., 2023d) attempts to integrate diverse user behaviors in recommendation environments, taking into account external social relationships. Focusing on simulating and evaluating user interactions with the recommenders, our framework Agent4Rec also falls into the category of simulation-oriented agents.
2310.11188	For stochastic MAB, Dudik et al. [29] first introduced the delay mechanism in the contextual bandit setting and proposed an algorithm achieving ğ’ªâ¢(Kâ¢logâ¡(Nâ¢T)â¢(d+T))ğ’ªğ¾ğ‘ğ‘‡ğ‘‘ğ‘‡\mathcal{O}(\sqrt{K\log(NT)}(d+\sqrt{T}))caligraphic_O ( square-root start_ARG italic_K roman_log ( italic_N italic_T ) end_ARG ( italic_d + square-root start_ARG italic_T end_ARG ) ) regret bound. Joulani et al. [30] investigated the impact of delayed feedback in online learning, particularly in scenarios like web advertisement and distributed learning, where feedback arrives with delays. The study revealed that delays lead to increased regret in a multiplicative manner for adversarial problems and additively for stochastic problems. The paper presented meta-algorithms that adapt existing non-delayed algorithms to handle delays efficiently, and also introduced modified versions of the UCB algorithm specifically designed for stochastic bandit problems with delayed feedback, offering lower complexity than the general meta-algorithms. Joulani et al. [31] studied delayed feedback under full information setting rather than adversarial bandits, and proved regret bound of ğ’ªâ¢((T+D)â¢lnâ¡N)ğ’ªğ‘‡ğ·ğ‘\mathcal{O}(\sqrt{(T+D)\ln{N}})caligraphic_O ( square-root start_ARG ( italic_T + italic_D ) roman_ln italic_N end_ARG ) by reducing the problem from non-delayed feedback full information setting, where D=âˆ‘t=1Tdtğ·superscriptsubscriptğ‘¡1ğ‘‡subscriptğ‘‘ğ‘¡D=\sum_{t=1}^{T}d_{t}italic_D = âˆ‘ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is delay sum. Vernade et al. [32] proposed the delayed stochastic bandit model based on the framework by Chapelle [33] with a partially observed feedback setting, assuming known delay distribution and bounded expected delay. Gael et al. [34] weakened the strong assumptions on the delay distributions with only a bound on the tail of the delay, and designed a UCB-based algorithm to solve it.
2310.11188	In this section, we formulate the problem as an adversarial bandit. Suppose the time is discretized into consecutive rounds. We consider an adversarial multi-armed bandit environment with Nğ‘Nitalic_N arms where the player selects an arm Atsubscriptğ´ğ‘¡A_{t}italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at round tğ‘¡titalic_t and the corresponding feedback is generated from Mğ‘€Mitalic_M individual users. We use the notation [K]={1,2,â‹¯,K}delimited-[]ğ¾12â‹¯ğ¾[K]=\{1,2,\cdots,K\}[ italic_K ] = { 1 , 2 , â‹¯ , italic_K } for brevity, then we define the set of arm indexes as ğ’©={i|iâˆˆ[N]}ğ’©conditional-setğ‘–ğ‘–delimited-[]ğ‘\mathcal{N}=\{i|i\in[N]\}caligraphic_N = { italic_i | italic_i âˆˆ [ italic_N ] }, the set of user indexes as â„³={j|jâˆˆ[M]}â„³conditional-setğ‘—ğ‘—delimited-[]ğ‘€\mathcal{M}=\{j|j\in[M]\}caligraphic_M = { italic_j | italic_j âˆˆ [ italic_M ] } and the set of round indexes as ğ’¯={t|tâˆˆ[T]}ğ’¯conditional-setğ‘¡ğ‘¡delimited-[]ğ‘‡\mathcal{T}=\{t|t\in[T]\}caligraphic_T = { italic_t | italic_t âˆˆ [ italic_T ] }. We use the loss rather than the reward to represent the feedback, denoted as lijâ¢(t)superscriptsubscriptğ‘™ğ‘–ğ‘—ğ‘¡l_{i}^{j}(t)italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_t ) for loss from user jğ‘—jitalic_j by selecting arm iğ‘–iitalic_i at round tğ‘¡titalic_t. The loss lAtjâ¢(t)superscriptsubscriptğ‘™subscriptğ´ğ‘¡ğ‘—ğ‘¡l_{A_{t}}^{j}(t)italic_l start_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_t ) is observed by the player after dtjsuperscriptsubscriptğ‘‘ğ‘¡ğ‘—d_{t}^{j}italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT rounds where the delay dtjsuperscriptsubscriptğ‘‘ğ‘¡ğ‘—d_{t}^{j}italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT is a positive integer (i.e., dtjâ‰¥1superscriptsubscriptğ‘‘ğ‘¡ğ‘—1d_{t}^{j}\geq 1italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT â‰¥ 1) and dmâ¢aâ¢x=maxâ¡{dtj}subscriptğ‘‘ğ‘šğ‘ğ‘¥superscriptsubscriptğ‘‘ğ‘¡ğ‘—d_{max}=\max\{d_{t}^{j}\}italic_d start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT = roman_max { italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT }. In other words, the player will observe a bunch of feedback losses {lAsjâ¢(s)|s+dsj=t}conditional-setsuperscriptsubscriptğ‘™subscriptğ´ğ‘ ğ‘—ğ‘ ğ‘ superscriptsubscriptğ‘‘ğ‘ ğ‘—ğ‘¡\{l_{A_{s}}^{j}(s)|s+d_{s}^{j}=t\}{ italic_l start_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_s ) | italic_s + italic_d start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT = italic_t } at round tğ‘¡titalic_t. Without loss of generality, we assume lijâ¢(t)âˆˆ[0,1]superscriptsubscriptğ‘™ğ‘–ğ‘—ğ‘¡01l_{i}^{j}(t)\in[0,1]italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_t ) âˆˆ [ 0 , 1 ]. Note that there is no restriction on the distribution of dtjsuperscriptsubscriptğ‘‘ğ‘¡ğ‘—d_{t}^{j}italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT for generality. The losses of arms and the delays are arbitrarily chosen by an adversary prior to the start of the game, which is known as an oblivious adversary.
2310.11188	In real-world scenarios, it is normally hard to acquire the information of terminal round and delays in advance. According to the Theorem 1, if with unknown Tğ‘‡Titalic_T, we cannot assign Î·ğœ‚\etaitalic_Î· in advance to guarantee the regret upper bound achieved through our analysis. However, in many real-world situations, the horizon index Tğ‘‡Titalic_T can barely be obtained beforehand. Therefore, a novel doubling trick is adopted to solve this problem. We define the number of missing feedback samples at round tğ‘¡titalic_t as Vtsubscriptğ‘‰ğ‘¡V_{t}italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. If one feedback sample lijâ¢(s)superscriptsubscriptğ‘™ğ‘–ğ‘—ğ‘ l_{i}^{j}(s)italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_s ) has not been observed at round tğ‘¡titalic_t (i.e. s+dsj>tğ‘ superscriptsubscriptğ‘‘ğ‘ ğ‘—ğ‘¡s+d_{s}^{j}>titalic_s + italic_d start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT > italic_t), it is considered to contribute one towards Vtsubscriptğ‘‰ğ‘¡V_{t}italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. We introduce the concept of epoch to divide the timeline, defining the Îµğœ€\varepsilonitalic_Îµ-th epoch ğ’¯Îµsubscriptğ’¯ğœ€\mathcal{T}_{\varepsilon}caligraphic_T start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT as the set of rounds that satisfying ğ’¯Îµ={t|2Îµâˆ’1â¢Mâ‰¤âˆ‘Ï„=1tVÏ„<2Îµâ¢M}subscriptğ’¯ğœ€conditional-setğ‘¡superscript2ğœ€1ğ‘€superscriptsubscriptğœ1ğ‘¡subscriptğ‘‰ğœsuperscript2ğœ€ğ‘€\mathcal{T}_{\varepsilon}=\left\{t|2^{\varepsilon-1}M\leq\sum_{\tau=1}^{t}V_{%
2310.11188	The emerging 5G communication technologies have been bringing significantly increasing connected mobile devices and data traffic. For example, YouTube consumes over 440000440000440000440000 terabytes of data daily, while blablabla. To reduce the backhaul transmission delay, mobile edge computing (MEC) is introduced, which caches content at the edge servers for the high quality-of-experience (QoE) of users. Due to the limitation of cache capacity, for better performance, edge servers should proactively cache the most popular content rather than reactively wait to receive the request sent by users. Some existing works [45, 46] assume the content popularity is known in advance, which in practice, however, is very unlikely to be true. Moreover, the served user group tends to be dynamic with users coming and leaving, leading to non-stationary content popularity. In addition, to improve caching performance, collecting user preference feedback (eg. a user like it or not) is necessary for QoE-oriented service [47, 48], which can hardly guarantee every user giving instant feedback and in turn brings about the delayed feedback issue.
2310.11188	The evaluation starts with the results of a stochastic bandit for reference, while the focus is on the subsequent adversarial bandits. We first evaluate the cumulative regrets of candidate algorithms with respect to round tğ‘¡titalic_t in Figure 2 under a stochastic bandit environment where the distribution of loss for each arm is fixed all along, then change the environment into our default adversarial one to assess the performance in Figure 3. Note that due to the limitation of the oracle setting, i.e. choosing a fixed arm throughout the procedure tends to bring about inferior performance in adversarial bandits, the results are primarily presented with a loss indicator rather than a regret one. Next, in order to investigate the influence of evolving distributions of loss on performance, we compare the total loss among three superior algorithms under adversarial bandit environments with diverse tran_num values in Figure 4-6, and demonstrate the specific tendency of loss when further increasing tran_num in Figure 7-8. In addition, we also qualitatively show the impacts of arm number Nğ‘Nitalic_N and user number Mğ‘€Mitalic_M on the performance of approaches in Figure 9. Note that the shade regions in those figures indicate the variation range with respect to two standard deviations.
2310.11188	Moreover, we extend the evolving factor tran_num to 50505050 and 100100100100, and depict the detailed tendency of loss in Figure 7 and Figure 8. In both figures, there is a superiority for the proposed methods over se all the time, but ducb approaches when the environment evolves more frequently, even slightly surpasses the proposed ones in the Figure 8 when tran_num is large. Despite the terrible stability of ducb with the largest shade region among all the methods, it possesses impressive adaptability in contrast to se in Figure 7 and Figure 8 when ducb constantly overwhelms se on average as the environment starts to evolve. It is interesting that in some circumstances such as tran_num=50tran_num50\textit{tran\_num}=50tran_num = 50, se even contributes higher loss than random method, which suggests very limited adaptability of se. The proposed methods mud and amud show good adaptability as a whole, while amud is considered to be a more stable one with less variation than mud. Comparing Figure 7 and Figure 8, it is illustrated that the larger tran_num (i.e. more frequently changed environment) and dmâ¢aâ¢xsubscriptğ‘‘ğ‘šğ‘ğ‘¥d_{max}italic_d start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT (i.e. less informed decision), the worse those algorithms perform, even approximating the random selection policy.
2310.11188v2	For stochastic MAB, Dudik et al. [29] first introduced the delay mechanism in the contextual bandit setting and proposed an algorithm achieving ğ’ªâ¢(Kâ¢logâ¡(Nâ¢T)â¢(d+T))ğ’ªğ¾ğ‘ğ‘‡ğ‘‘ğ‘‡\mathcal{O}(\sqrt{K\log(NT)}(d+\sqrt{T}))caligraphic_O ( square-root start_ARG italic_K roman_log ( italic_N italic_T ) end_ARG ( italic_d + square-root start_ARG italic_T end_ARG ) ) regret bound. Joulani et al. [30] investigated the impact of delayed feedback in online learning, particularly in scenarios like web advertisement and distributed learning, where feedback arrives with delays. The study revealed that delays lead to increased regret in a multiplicative manner for adversarial problems and additively for stochastic problems. The paper presented meta-algorithms that adapt existing non-delayed algorithms to handle delays efficiently, and also introduced modified versions of the UCB algorithm specifically designed for stochastic bandit problems with delayed feedback, offering lower complexity than the general meta-algorithms. Joulani et al. [31] studied delayed feedback under full information setting rather than adversarial bandits, and proved regret bound of ğ’ªâ¢((T+D)â¢lnâ¡N)ğ’ªğ‘‡ğ·ğ‘\mathcal{O}(\sqrt{(T+D)\ln{N}})caligraphic_O ( square-root start_ARG ( italic_T + italic_D ) roman_ln italic_N end_ARG ) by reducing the problem from non-delayed feedback full information setting, where D=âˆ‘t=1Tdtğ·superscriptsubscriptğ‘¡1ğ‘‡subscriptğ‘‘ğ‘¡D=\sum_{t=1}^{T}d_{t}italic_D = âˆ‘ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is delay sum. Vernade et al. [32] proposed the delayed stochastic bandit model based on the framework by Chapelle [33] with a partially observed feedback setting, assuming known delay distribution and bounded expected delay. Gael et al. [34] weakened the strong assumptions on the delay distributions with only a bound on the tail of the delay, and designed a UCB-based algorithm to solve it.
2310.11188v2	In this section, we formulate the problem as an adversarial bandit. Suppose the time is discretized into consecutive rounds. We consider an adversarial multi-armed bandit environment with Nğ‘Nitalic_N arms where the player selects an arm Atsubscriptğ´ğ‘¡A_{t}italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at round tğ‘¡titalic_t and the corresponding feedback is generated from Mğ‘€Mitalic_M individual users. We use the notation [K]={1,2,â‹¯,K}delimited-[]ğ¾12â‹¯ğ¾[K]=\{1,2,\cdots,K\}[ italic_K ] = { 1 , 2 , â‹¯ , italic_K } for brevity, then we define the set of arm indexes as ğ’©={i|iâˆˆ[N]}ğ’©conditional-setğ‘–ğ‘–delimited-[]ğ‘\mathcal{N}=\{i|i\in[N]\}caligraphic_N = { italic_i | italic_i âˆˆ [ italic_N ] }, the set of user indexes as â„³={j|jâˆˆ[M]}â„³conditional-setğ‘—ğ‘—delimited-[]ğ‘€\mathcal{M}=\{j|j\in[M]\}caligraphic_M = { italic_j | italic_j âˆˆ [ italic_M ] } and the set of round indexes as ğ’¯={t|tâˆˆ[T]}ğ’¯conditional-setğ‘¡ğ‘¡delimited-[]ğ‘‡\mathcal{T}=\{t|t\in[T]\}caligraphic_T = { italic_t | italic_t âˆˆ [ italic_T ] }. We use the loss rather than the reward to represent the feedback, denoted as lijâ¢(t)superscriptsubscriptğ‘™ğ‘–ğ‘—ğ‘¡l_{i}^{j}(t)italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_t ) for loss from user jğ‘—jitalic_j by selecting arm iğ‘–iitalic_i at round tğ‘¡titalic_t. The loss lAtjâ¢(t)superscriptsubscriptğ‘™subscriptğ´ğ‘¡ğ‘—ğ‘¡l_{A_{t}}^{j}(t)italic_l start_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_t ) is observed by the player after dtjsuperscriptsubscriptğ‘‘ğ‘¡ğ‘—d_{t}^{j}italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT rounds where the delay dtjsuperscriptsubscriptğ‘‘ğ‘¡ğ‘—d_{t}^{j}italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT is a positive integer (i.e., dtjâ‰¥1superscriptsubscriptğ‘‘ğ‘¡ğ‘—1d_{t}^{j}\geq 1italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT â‰¥ 1) and dmâ¢aâ¢x=maxâ¡{dtj}subscriptğ‘‘ğ‘šğ‘ğ‘¥superscriptsubscriptğ‘‘ğ‘¡ğ‘—d_{max}=\max\{d_{t}^{j}\}italic_d start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT = roman_max { italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT }. In other words, the player will observe a bunch of feedback losses {lAsjâ¢(s)|s+dsj=t}conditional-setsuperscriptsubscriptğ‘™subscriptğ´ğ‘ ğ‘—ğ‘ ğ‘ superscriptsubscriptğ‘‘ğ‘ ğ‘—ğ‘¡\{l_{A_{s}}^{j}(s)|s+d_{s}^{j}=t\}{ italic_l start_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_s ) | italic_s + italic_d start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT = italic_t } at round tğ‘¡titalic_t. Without loss of generality, we assume lijâ¢(t)âˆˆ[0,1]superscriptsubscriptğ‘™ğ‘–ğ‘—ğ‘¡01l_{i}^{j}(t)\in[0,1]italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_t ) âˆˆ [ 0 , 1 ]. Note that there is no restriction on the distribution of dtjsuperscriptsubscriptğ‘‘ğ‘¡ğ‘—d_{t}^{j}italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT for generality. The losses of arms and the delays are arbitrarily chosen by an adversary prior to the start of the game, which is known as an oblivious adversary.
2310.11188v2	In real-world scenarios, it is normally hard to acquire the information of terminal round and delays in advance. According to the Theorem 1, if with unknown Tğ‘‡Titalic_T, we cannot assign Î·ğœ‚\etaitalic_Î· in advance to guarantee the regret upper bound achieved through our analysis. However, in many real-world situations, the horizon index Tğ‘‡Titalic_T can barely be obtained beforehand. Therefore, a novel doubling trick is adopted to solve this problem. We define the number of missing feedback samples at round tğ‘¡titalic_t as Vtsubscriptğ‘‰ğ‘¡V_{t}italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. If one feedback sample lijâ¢(s)superscriptsubscriptğ‘™ğ‘–ğ‘—ğ‘ l_{i}^{j}(s)italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_s ) has not been observed at round tğ‘¡titalic_t (i.e. s+dsj>tğ‘ superscriptsubscriptğ‘‘ğ‘ ğ‘—ğ‘¡s+d_{s}^{j}>titalic_s + italic_d start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT > italic_t), it is considered to contribute one towards Vtsubscriptğ‘‰ğ‘¡V_{t}italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. We introduce the concept of epoch to divide the timeline, defining the Îµğœ€\varepsilonitalic_Îµ-th epoch ğ’¯Îµsubscriptğ’¯ğœ€\mathcal{T}_{\varepsilon}caligraphic_T start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT as the set of rounds that satisfying ğ’¯Îµ={t|2Îµâˆ’1â¢Mâ‰¤âˆ‘Ï„=1tVÏ„<2Îµâ¢M}subscriptğ’¯ğœ€conditional-setğ‘¡superscript2ğœ€1ğ‘€superscriptsubscriptğœ1ğ‘¡subscriptğ‘‰ğœsuperscript2ğœ€ğ‘€\mathcal{T}_{\varepsilon}=\left\{t|2^{\varepsilon-1}M\leq\sum_{\tau=1}^{t}V_{%
2310.11188v2	The emerging 5G communication technologies have been bringing significantly increasing connected mobile devices and data traffic. For example, YouTube consumes over 440000440000440000440000 terabytes of data daily, while blablabla. To reduce the backhaul transmission delay, mobile edge computing (MEC) is introduced, which caches content at the edge servers for the high quality-of-experience (QoE) of users. Due to the limitation of cache capacity, for better performance, edge servers should proactively cache the most popular content rather than reactively wait to receive the request sent by users. Some existing works [45, 46] assume the content popularity is known in advance, which in practice, however, is very unlikely to be true. Moreover, the served user group tends to be dynamic with users coming and leaving, leading to non-stationary content popularity. In addition, to improve caching performance, collecting user preference feedback (eg. a user like it or not) is necessary for QoE-oriented service [47, 48], which can hardly guarantee every user giving instant feedback and in turn brings about the delayed feedback issue.
2310.11188v2	The evaluation starts with the results of a stochastic bandit for reference, while the focus is on the subsequent adversarial bandits. We first evaluate the cumulative regrets of candidate algorithms with respect to round tğ‘¡titalic_t in Figure 2 under a stochastic bandit environment where the distribution of loss for each arm is fixed all along, then change the environment into our default adversarial one to assess the performance in Figure 3. Note that due to the limitation of the oracle setting, i.e. choosing a fixed arm throughout the procedure tends to bring about inferior performance in adversarial bandits, the results are primarily presented with a loss indicator rather than a regret one. Next, in order to investigate the influence of evolving distributions of loss on performance, we compare the total loss among three superior algorithms under adversarial bandit environments with diverse tran_num values in Figure 4-6, and demonstrate the specific tendency of loss when further increasing tran_num in Figure 7-8. In addition, we also qualitatively show the impacts of arm number Nğ‘Nitalic_N and user number Mğ‘€Mitalic_M on the performance of approaches in Figure 9. Note that the shade regions in those figures indicate the variation range with respect to two standard deviations.
2310.11188v2	Moreover, we extend the evolving factor tran_num to 50505050 and 100100100100, and depict the detailed tendency of loss in Figure 7 and Figure 8. In both figures, there is a superiority for the proposed methods over se all the time, but ducb approaches when the environment evolves more frequently, even slightly surpasses the proposed ones in the Figure 8 when tran_num is large. Despite the terrible stability of ducb with the largest shade region among all the methods, it possesses impressive adaptability in contrast to se in Figure 7 and Figure 8 when ducb constantly overwhelms se on average as the environment starts to evolve. It is interesting that in some circumstances such as tran_num=50tran_num50\textit{tran\_num}=50tran_num = 50, se even contributes higher loss than random method, which suggests very limited adaptability of se. The proposed methods mud and amud show good adaptability as a whole, while amud is considered to be a more stable one with less variation than mud. Comparing Figure 7 and Figure 8, it is illustrated that the larger tran_num (i.e. more frequently changed environment) and dmâ¢aâ¢xsubscriptğ‘‘ğ‘šğ‘ğ‘¥d_{max}italic_d start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT (i.e. less informed decision), the worse those algorithms perform, even approximating the random selection policy.
2402.15235	Unlike previous studies focused on user/item simulation with agents, we propose a new multi-agent collaboration framework for recommendation MACRec. In this framework, agents with different abilities, work collaboratively are involved to tackle specific recommendation tasks.
2405.14249	Unlike previous work on breakdown mitigation, our proposed method aims to automatically identify breakdowns and give insights on how to prevent them from happening, without the involvement of real users. Modifying the conversation agent based on these insights should reduce the use of repair strategies or even eliminate their need altogether.
2405.14249	MÃ¶ller etÂ al. (2006) proposed the MeMo system to automatically assess the usability of spoken dialogue systems. The system uses mental models to simulate users which can generate different types of errors. The generation of these errors is facilitated by the annotation of errors in historical dialogues. The aim of MeMo is to assess the impact of errors on the user experience rather than their detection. Indeed, the detection of errors is facilitated by the fact that they are generated by the user simulator.
2405.14249v1	Unlike previous work on breakdown mitigation, our proposed method aims to automatically identify breakdowns and give insights on how to prevent them from happening, without the involvement of real users. Modifying the conversation agent based on these insights should reduce the use of repair strategies or even eliminate their need altogether.
2405.14249v1	MÃ¶ller etÂ al. (2006) proposed the MeMo system to automatically assess the usability of spoken dialogue systems. The system uses mental models to simulate users which can generate different types of errors. The generation of these errors is facilitated by the annotation of errors in historical dialogues. The aim of MeMo is to assess the impact of errors on the user experience rather than their detection. Indeed, the detection of errors is facilitated by the fact that they are generated by the user simulator.
2410.20027	While prior research has primarily focused on enhancing either the recommendation agent or the user agent individually, the collaborative interaction between the two has often been overlooked.
2410.20027	Existing research primarily focuses on optimizing either the recommendation agent or the user agent separately, overlooking the critical role of the feedback loop between the user and the recommender.
2410.20027	AFL relies exclusively on memory to record interactions and update both agents, making it simple and broadly applicable. It is not limited to any specific agents and can be integrated with almost any recommendation or user agent equipped with memory.
2410.20027	If the recommendation agent relies more on positional information rather than user preferences, the â€œFirstâ€ and â€œLastâ€ settings are expected to significantly outperform the â€œRandomâ€ setting, as the agent only needs to blindly output either the first or the last item.
2410.20027	(3) In conclusion, AFL demonstrates resistance to location bias by learning user preferences rather than location information through the collaboration of the recommendation agent and the user agent.
2410.20027	Our framework is simple and generic: it is not limited to the specific agents designed in this paper, but is intended to serve as a guideline for fostering collaborative and mutually beneficial interactions between the recommendation agent and the user agent.
2411.07569	We confirm that the additional work is original and has not been published elsewhere, nor is it currently under consideration for publication elsewhere.
2411.07569v1	We confirm that the additional work is original and has not been published elsewhere, nor is it currently under consideration for publication elsewhere.
2503.21676	An important methodological challenge in our study is to isolate knowledge from other model capabilities. To address this, we adapt the synthetic biography dataset of Allen-Zhu and Li (2023), which offers several appealing properties. First, all facts in the dataset are atomic, meaning no fact can be derived from others, thus allowing us to disambiguate knowledge recall from other abilities like reasoning. Second, it is fully synthetic, which allows for precise control of the data distributional properties, while employing natural language with realistic statistics. For example, sufficient textual variation is crucial for enabling genuine knowledge acquisition rather than mere memorization (Allen-Zhu and Li, 2023), cf. FigureÂ A in the appendix. Third, and most importantly, previous work has established that relatively small language models trained on this dataset (Allen-Zhu and Li, 2023) store and use knowledge in a similar way to large language models (Geva etÂ al., 2021; Meng etÂ al., 2022; Geva etÂ al., 2023; Nanda etÂ al., 2023b).
2503.21676	A consequence of that hypothesis is that a model with learned attention patterns should learn faster than one with pre-learning patterns. To test this, we design the following attention patching experiment (FigureÂ 3, left). We first train a reference model and save reference checkpoints from it at various stages of training. We then restart training a model from scratch, but replace the modelâ€™s attention patterns111We call attention patterns the softmax of the attention logits. with those produced from one of the reference checkpoints. This means that all the token-to-token interactions are specified by the reference model and that the modified model only learns token-wise feedforward transformations. We expect the quality of the attention patterns, measured by how easy it is to learn the task with them, provided by the reference model to progressively improve as they are taken closer to the plateau end. This is what we observe empirically (FigureÂ 3, middle), and the plateau disappears when providing the model with learned attention patterns (i.e. post-plateau patterns). Interestingly, the attention patterns acquired early during learning (e.g., at steps 500500500500 and 1111k) are significantly worse than the pre-learning ones, likely due to initial focus on predicting the attribute value distribution conditioned on the attribute type only. This attention on attribute type tokens rather than on name tokens slows down learning, as per our argument from the last paragraph.
2503.21676	For the sake of our analysis, the template generation requires extra care, as we want all the information needed to predict the attribute value and as we want to evaluate the model on sentences it has never seen. Such considerations were not needed in Allen-Zhu and Li (2023), so we had to adapt their setup. In our dataset, templates are generated prior to training with the assistance of a large language model, using the prompting scheme in FigureÂ B. This yields 25252525 distinct templates per attribute type. As discussed in SectionÂ 1.2, we (manually) ensure that the individualâ€™s name and information identifying the attribute type precede the attribute value, allowing a model with perfect knowledge to achieve zero loss. For each individual, we pick 20202020 templates for training and keep 5555 for evaluation, ensuring the model encounters novel template-individual combinations during evaluation, thus measuring knowledge rather than memorization. Additionally, we introduce special tokens for tags (like name or birth date) in the templates and replace these tags by the desired content only after tokenization. This way, we can make sure that, after this manipulation, the tokens coming from names or attribute values directly appear in the token sequence (e.g. "1990." will be tokenized as "[1990][.]" and not "[1990.]" as it would usually be). Such a precaution facilitates analysis as we can be sure that each of the name or attribute value tokens only contain this information, and not some unrelated semantic information about the sentence.
2503.21676	The experiments in FigureÂ 2 use 5555 different seeds. However, given the low variance of the results, we preferred to use compute to explore more diverse hyperparameter configurations rather than performing our analysis on more seeds and ended up using a single seed for the rest of the experiments.
2503.21676	In our analysis, the modelâ€™s attention patterns during learning, we examine which tokens the network attends to when processing or predicting specific tokens. This approach is motivated by prior work (discussed in SectionÂ 2.2 and visualized in FigureÂ E) that identified specific attention-based circuits for factual recall tasks, each with distinct signatures observable through attention patterns. We focus on the following circuits and their signatures:
2503.21676v2	An important methodological challenge in our study is to isolate knowledge from other model capabilities. To address this, we adapt the synthetic biography dataset of Allen-Zhu and Li (2023), which offers several appealing properties. First, all facts in the dataset are atomic, meaning no fact can be derived from others, thus allowing us to disambiguate knowledge recall from other abilities like reasoning. Second, it is fully synthetic, which allows for precise control of the data distributional properties, while employing natural language with realistic statistics. For example, sufficient textual variation is crucial for enabling genuine knowledge acquisition rather than mere memorization (Allen-Zhu and Li, 2023), cf. FigureÂ A in the appendix. Third, and most importantly, previous work has established that relatively small language models trained on this dataset (Allen-Zhu and Li, 2023) store and use knowledge in a similar way to large language models (Geva etÂ al., 2021; Meng etÂ al., 2022; Geva etÂ al., 2023; Nanda etÂ al., 2023b).
2503.21676v2	A consequence of that hypothesis is that a model with learned attention patterns should learn faster than one with pre-learning patterns. To test this, we design the following attention patching experiment (FigureÂ 3, left). We first train a reference model and save reference checkpoints from it at various stages of training. We then restart training a model from scratch, but replace the modelâ€™s attention patterns111We call attention patterns the softmax of the attention logits. with those produced from one of the reference checkpoints. This means that all the token-to-token interactions are specified by the reference model and that the modified model only learns token-wise feedforward transformations. We expect the quality of the attention patterns, measured by how easy it is to learn the task with them, provided by the reference model to progressively improve as they are taken closer to the plateau end. This is what we observe empirically (FigureÂ 3, middle), and the plateau disappears when providing the model with learned attention patterns (i.e. post-plateau patterns). Interestingly, the attention patterns acquired early during learning (e.g., at steps 500500500500 and 1111k) are significantly worse than the pre-learning ones, likely due to initial focus on predicting the attribute value distribution conditioned on the attribute type only. This attention on attribute type tokens rather than on name tokens slows down learning, as per our argument from the last paragraph.
2503.21676v2	For the sake of our analysis, the template generation requires extra care, as we want all the information needed to predict the attribute value and as we want to evaluate the model on sentences it has never seen. Such considerations were not needed in Allen-Zhu and Li (2023), so we had to adapt their setup. In our dataset, templates are generated prior to training with the assistance of a large language model, using the prompting scheme in FigureÂ B. This yields 25252525 distinct templates per attribute type. As discussed in SectionÂ 1.2, we (manually) ensure that the individualâ€™s name and information identifying the attribute type precede the attribute value, allowing a model with perfect knowledge to achieve zero loss. For each individual, we pick 20202020 templates for training and keep 5555 for evaluation, ensuring the model encounters novel template-individual combinations during evaluation, thus measuring knowledge rather than memorization. Additionally, we introduce special tokens for tags (like name or birth date) in the templates and replace these tags by the desired content only after tokenization. This way, we can make sure that, after this manipulation, the tokens coming from names or attribute values directly appear in the token sequence (e.g. "1990." will be tokenized as "[1990][.]" and not "[1990.]" as it would usually be). Such a precaution facilitates analysis as we can be sure that each of the name or attribute value tokens only contain this information, and not some unrelated semantic information about the sentence.
2503.21676v2	The experiments in FigureÂ 2 use 5555 different seeds. However, given the low variance of the results, we preferred to use compute to explore more diverse hyperparameter configurations rather than performing our analysis on more seeds and ended up using a single seed for the rest of the experiments.
2503.21676v2	In our analysis, the modelâ€™s attention patterns during learning, we examine which tokens the network attends to when processing or predicting specific tokens. This approach is motivated by prior work (discussed in SectionÂ 2.2 and visualized in FigureÂ E) that identified specific attention-based circuits for factual recall tasks, each with distinct signatures observable through attention patterns. We focus on the following circuits and their signatures:
